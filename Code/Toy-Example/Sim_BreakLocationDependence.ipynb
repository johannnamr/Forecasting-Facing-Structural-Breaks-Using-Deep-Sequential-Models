{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0de3307",
   "metadata": {},
   "source": [
    "# Simulation: Dependence on Break Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8bbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017af60",
   "metadata": {},
   "source": [
    "Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb283dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Meier/Dropbox (Institut f√ºr Statistik)/Structural Breaks + DL/Figures/Results/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438102cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RNN' # 'RNN', 'LSTM', 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cf74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 1000           # number of repetitions\n",
    "sim_length = 500      # length of simulated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e291bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1           # proportion of test set\n",
    "lags = 1                  # number of lags as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aad240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1b285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "input_dim = lags                  # number of lagged features in X\n",
    "hidden_dim = 10                   # number of hidden nodes per layer\n",
    "layer_dim = 1                     # number of layers\n",
    "output_dim = 1                    # output dimension (1 for univariate output)\n",
    "dropout = 0                       # dropout proportion (only before the last sequential layer)\n",
    "learning_rate = 1e-3              # learning rate for Adam optimizer\n",
    "weight_decay = 1e-6               # weight decay for Adam optimizer\n",
    "\n",
    "# save model parameters in dict\n",
    "model_params = {'input_dim': input_dim, 'hidden_dim' : hidden_dim,'layer_dim' : layer_dim, 'output_dim' : output_dim, 'dropout_prob' : dropout}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82bcfc",
   "metadata": {},
   "source": [
    "Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060f7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        self.rnn.flatten_parameters() # ------------------------------------------------------------------\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        self.lstm.flatten_parameters() # ------------------------------------------------------------------\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        self.gru.flatten_parameters() # ------------------------------------------------------------------\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6523ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"rnn\": RNNModel,\n",
    "        \"lstm\": LSTMModel,\n",
    "        \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721b2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, train_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        \n",
    "        # train on GPU\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([len(y_batch), -1, n_features]).to(device)\n",
    "                y_batch = y_batch.view([len(y_batch), -1]).to(device)\n",
    "                b_loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(b_loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, best_model, test_loader, batch_size=1, n_features=1):\n",
    "        # evaluate on GPU\n",
    "        device = torch.device('cuda')\n",
    "        model = deepcopy(best_model)\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.view([batch_size, -1]).to(device)\n",
    "                model.eval()\n",
    "                yhat = model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().cpu().numpy())\n",
    "                values.append(y_test.to(device).detach().cpu().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28bc2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_dl(predictions, values):\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds})\n",
    "    df_result = df_result.sort_index()\n",
    "    return df_result\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    return {'rmse' : mean_squared_error(df.value, df.prediction)**0.5,\n",
    "            'mae' : mean_absolute_error(df.value, df.prediction),\n",
    "            'mape': mean_absolute_percentage_error(df.value, df.prediction),\n",
    "            'r2' : r2_score(df.value, df.prediction)}\n",
    "\n",
    "def time_format(seconds: int):\n",
    "    if seconds is not None:\n",
    "        seconds = int(seconds)\n",
    "        d = seconds // (3600 * 24)\n",
    "        h = seconds // 3600 % 24\n",
    "        m = seconds % 3600 // 60\n",
    "        s = seconds % 3600 % 60\n",
    "        if d > 0:\n",
    "            return '{:02d}D {:02d}H {:02d}m {:02d}s'.format(d, h, m, s)\n",
    "        elif h > 0:\n",
    "            return '{:02d}H {:02d}m {:02d}s'.format(h, m, s)\n",
    "        elif m > 0:\n",
    "            return '{:02d}m {:02d}s'.format(m, s)\n",
    "        elif s > 0:\n",
    "            return '{:02d}s'.format(s)\n",
    "    return '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c9e0d",
   "metadata": {},
   "source": [
    "Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df28ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over break locations\n",
    "tau = np.arange(0,1,0.05)\n",
    "data = []\n",
    "for j in range(len(tau)):\n",
    "        \n",
    "    # simulate constant data (with mean break from 0 to 0.5)\n",
    "    if j == 0:\n",
    "        data_temp = 0.5*np.ones(int(sim_length)+lags)\n",
    "    else:\n",
    "        #data_temp = np.concatenate((np.zeros(int(np.round(sim_length*tau[j],0)+lags)),0.5*np.ones(int(np.round(sim_length*(1-tau[j]),0)))),axis=0)\n",
    "        data_temp = np.concatenate((np.zeros(int(np.round((1-test_size)*sim_length*tau[j],0)+lags)),0.5*np.ones(int(np.round((1-test_size)*sim_length*(1-tau[j])+test_size*sim_length,0)))),axis=0)\n",
    "\n",
    "        \n",
    "    # split into training and test data\n",
    "    X_temp = data_temp[:-1]\n",
    "    y_temp = data_temp[1:]\n",
    "    data.append(train_test_split(X_temp, y_temp, test_size=test_size, shuffle=False)) # X_train, X_test, y_train, y_test for every j in tau "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802eefef",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae26d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation start: Wed Oct  5 15:52:08 2022\n",
      "Repetition:  1\n",
      "Elapsed: 16s\n",
      "Repetition:  2\n",
      "Elapsed: 31s\n",
      "Repetition:  3\n",
      "Elapsed: 47s\n",
      "Repetition:  4\n",
      "Elapsed: 01m 02s\n",
      "Repetition:  5\n",
      "Elapsed: 01m 17s\n",
      "Repetition:  6\n",
      "Elapsed: 01m 32s\n",
      "Repetition:  7\n",
      "Elapsed: 01m 48s\n",
      "Repetition:  8\n",
      "Elapsed: 02m 03s\n",
      "Repetition:  9\n",
      "Elapsed: 02m 19s\n",
      "Repetition:  10\n",
      "Elapsed: 02m 34s\n",
      "Repetition:  11\n",
      "Repetition:  12\n",
      "Repetition:  13\n",
      "Repetition:  14\n",
      "Repetition:  15\n",
      "Repetition:  16\n",
      "Repetition:  17\n",
      "Repetition:  18\n",
      "Repetition:  19\n",
      "Repetition:  20\n",
      "Repetition:  21\n",
      "Repetition:  22\n",
      "Repetition:  23\n",
      "Repetition:  24\n",
      "Repetition:  25\n",
      "Repetition:  26\n",
      "Repetition:  27\n",
      "Repetition:  28\n",
      "Repetition:  29\n",
      "Repetition:  30\n",
      "Repetition:  31\n",
      "Repetition:  32\n",
      "Repetition:  33\n",
      "Repetition:  34\n",
      "Repetition:  35\n",
      "Repetition:  36\n",
      "Repetition:  37\n",
      "Repetition:  38\n",
      "Repetition:  39\n",
      "Repetition:  40\n",
      "Repetition:  41\n",
      "Repetition:  42\n",
      "Repetition:  43\n",
      "Repetition:  44\n",
      "Repetition:  45\n",
      "Repetition:  46\n",
      "Repetition:  47\n",
      "Repetition:  48\n",
      "Repetition:  49\n",
      "Repetition:  50\n",
      "Repetition:  51\n",
      "Elapsed: 13m 20s\n",
      "Repetition:  52\n",
      "Repetition:  53\n",
      "Repetition:  54\n",
      "Repetition:  55\n",
      "Repetition:  56\n",
      "Repetition:  57\n",
      "Repetition:  58\n",
      "Repetition:  59\n",
      "Repetition:  60\n",
      "Repetition:  61\n",
      "Repetition:  62\n",
      "Repetition:  63\n",
      "Repetition:  64\n",
      "Repetition:  65\n",
      "Repetition:  66\n",
      "Repetition:  67\n",
      "Repetition:  68\n",
      "Repetition:  69\n",
      "Repetition:  70\n",
      "Repetition:  71\n",
      "Repetition:  72\n",
      "Repetition:  73\n",
      "Repetition:  74\n",
      "Repetition:  75\n",
      "Repetition:  76\n",
      "Repetition:  77\n",
      "Repetition:  78\n",
      "Repetition:  79\n",
      "Repetition:  80\n",
      "Repetition:  81\n",
      "Repetition:  82\n",
      "Repetition:  83\n",
      "Repetition:  84\n",
      "Repetition:  85\n",
      "Repetition:  86\n",
      "Repetition:  87\n",
      "Repetition:  88\n",
      "Repetition:  89\n",
      "Repetition:  90\n",
      "Repetition:  91\n",
      "Repetition:  92\n",
      "Repetition:  93\n",
      "Repetition:  94\n",
      "Repetition:  95\n",
      "Repetition:  96\n",
      "Repetition:  97\n",
      "Repetition:  98\n",
      "Repetition:  99\n",
      "Repetition:  100\n",
      "Repetition:  101\n",
      "Elapsed: 26m 46s\n",
      "Repetition:  102\n",
      "Repetition:  103\n",
      "Repetition:  104\n",
      "Repetition:  105\n",
      "Repetition:  106\n",
      "Repetition:  107\n",
      "Repetition:  108\n",
      "Repetition:  109\n",
      "Repetition:  110\n",
      "Repetition:  111\n",
      "Repetition:  112\n",
      "Repetition:  113\n",
      "Repetition:  114\n",
      "Repetition:  115\n",
      "Repetition:  116\n",
      "Repetition:  117\n",
      "Repetition:  118\n",
      "Repetition:  119\n",
      "Repetition:  120\n",
      "Repetition:  121\n",
      "Repetition:  122\n",
      "Repetition:  123\n",
      "Repetition:  124\n",
      "Repetition:  125\n",
      "Repetition:  126\n",
      "Repetition:  127\n",
      "Repetition:  128\n",
      "Repetition:  129\n",
      "Repetition:  130\n",
      "Repetition:  131\n",
      "Repetition:  132\n",
      "Repetition:  133\n",
      "Repetition:  134\n",
      "Repetition:  135\n",
      "Repetition:  136\n",
      "Repetition:  137\n",
      "Repetition:  138\n",
      "Repetition:  139\n",
      "Repetition:  140\n",
      "Repetition:  141\n",
      "Repetition:  142\n",
      "Repetition:  143\n",
      "Repetition:  144\n",
      "Repetition:  145\n",
      "Repetition:  146\n",
      "Repetition:  147\n",
      "Repetition:  148\n",
      "Repetition:  149\n",
      "Repetition:  150\n",
      "Repetition:  151\n",
      "Elapsed: 39m 58s\n",
      "Repetition:  152\n",
      "Repetition:  153\n",
      "Repetition:  154\n",
      "Repetition:  155\n",
      "Repetition:  156\n",
      "Repetition:  157\n",
      "Repetition:  158\n",
      "Repetition:  159\n",
      "Repetition:  160\n",
      "Repetition:  161\n",
      "Repetition:  162\n",
      "Repetition:  163\n",
      "Repetition:  164\n",
      "Repetition:  165\n",
      "Repetition:  166\n",
      "Repetition:  167\n",
      "Repetition:  168\n",
      "Repetition:  169\n",
      "Repetition:  170\n",
      "Repetition:  171\n",
      "Repetition:  172\n",
      "Repetition:  173\n",
      "Repetition:  174\n",
      "Repetition:  175\n",
      "Repetition:  176\n",
      "Repetition:  177\n",
      "Repetition:  178\n",
      "Repetition:  179\n",
      "Repetition:  180\n",
      "Repetition:  181\n",
      "Repetition:  182\n",
      "Repetition:  183\n",
      "Repetition:  184\n",
      "Repetition:  185\n",
      "Repetition:  186\n",
      "Repetition:  187\n",
      "Repetition:  188\n",
      "Repetition:  189\n",
      "Repetition:  190\n",
      "Repetition:  191\n",
      "Repetition:  192\n",
      "Repetition:  193\n",
      "Repetition:  194\n",
      "Repetition:  195\n",
      "Repetition:  196\n",
      "Repetition:  197\n",
      "Repetition:  198\n",
      "Repetition:  199\n",
      "Repetition:  200\n",
      "Repetition:  201\n",
      "Elapsed: 52m 56s\n",
      "Repetition:  202\n",
      "Repetition:  203\n",
      "Repetition:  204\n",
      "Repetition:  205\n",
      "Repetition:  206\n",
      "Repetition:  207\n",
      "Repetition:  208\n",
      "Repetition:  209\n",
      "Repetition:  210\n",
      "Repetition:  211\n",
      "Repetition:  212\n",
      "Repetition:  213\n",
      "Repetition:  214\n",
      "Repetition:  215\n",
      "Repetition:  216\n",
      "Repetition:  217\n",
      "Repetition:  218\n",
      "Repetition:  219\n",
      "Repetition:  220\n",
      "Repetition:  221\n",
      "Repetition:  222\n",
      "Repetition:  223\n",
      "Repetition:  224\n",
      "Repetition:  225\n",
      "Repetition:  226\n",
      "Repetition:  227\n",
      "Repetition:  228\n",
      "Repetition:  229\n",
      "Repetition:  230\n",
      "Repetition:  231\n",
      "Repetition:  232\n",
      "Repetition:  233\n",
      "Repetition:  234\n",
      "Repetition:  235\n",
      "Repetition:  236\n",
      "Repetition:  237\n",
      "Repetition:  238\n",
      "Repetition:  239\n",
      "Repetition:  240\n",
      "Repetition:  241\n",
      "Repetition:  242\n",
      "Repetition:  243\n",
      "Repetition:  244\n",
      "Repetition:  245\n",
      "Repetition:  246\n",
      "Repetition:  247\n",
      "Repetition:  248\n",
      "Repetition:  249\n",
      "Repetition:  250\n",
      "Repetition:  251\n",
      "Elapsed: 01H 05m 53s\n",
      "Repetition:  252\n",
      "Repetition:  253\n",
      "Repetition:  254\n",
      "Repetition:  255\n",
      "Repetition:  256\n",
      "Repetition:  257\n",
      "Repetition:  258\n",
      "Repetition:  259\n",
      "Repetition:  260\n",
      "Repetition:  261\n",
      "Repetition:  262\n",
      "Repetition:  263\n",
      "Repetition:  264\n",
      "Repetition:  265\n",
      "Repetition:  266\n",
      "Repetition:  267\n",
      "Repetition:  268\n",
      "Repetition:  269\n",
      "Repetition:  270\n",
      "Repetition:  271\n",
      "Repetition:  272\n",
      "Repetition:  273\n",
      "Repetition:  274\n",
      "Repetition:  275\n",
      "Repetition:  276\n",
      "Repetition:  277\n",
      "Repetition:  278\n",
      "Repetition:  279\n",
      "Repetition:  280\n",
      "Repetition:  281\n",
      "Repetition:  282\n",
      "Repetition:  283\n",
      "Repetition:  284\n",
      "Repetition:  285\n",
      "Repetition:  286\n",
      "Repetition:  287\n",
      "Repetition:  288\n",
      "Repetition:  289\n",
      "Repetition:  290\n",
      "Repetition:  291\n",
      "Repetition:  292\n",
      "Repetition:  293\n",
      "Repetition:  294\n",
      "Repetition:  295\n",
      "Repetition:  296\n",
      "Repetition:  297\n",
      "Repetition:  298\n",
      "Repetition:  299\n",
      "Repetition:  300\n",
      "Repetition:  301\n",
      "Elapsed: 01H 18m 50s\n",
      "Repetition:  302\n",
      "Repetition:  303\n",
      "Repetition:  304\n",
      "Repetition:  305\n",
      "Repetition:  306\n",
      "Repetition:  307\n",
      "Repetition:  308\n",
      "Repetition:  309\n",
      "Repetition:  310\n",
      "Repetition:  311\n",
      "Repetition:  312\n",
      "Repetition:  313\n",
      "Repetition:  314\n",
      "Repetition:  315\n",
      "Repetition:  316\n",
      "Repetition:  317\n",
      "Repetition:  318\n",
      "Repetition:  319\n",
      "Repetition:  320\n",
      "Repetition:  321\n",
      "Repetition:  322\n",
      "Repetition:  323\n",
      "Repetition:  324\n",
      "Repetition:  325\n",
      "Repetition:  326\n",
      "Repetition:  327\n",
      "Repetition:  328\n",
      "Repetition:  329\n",
      "Repetition:  330\n",
      "Repetition:  331\n",
      "Repetition:  332\n",
      "Repetition:  333\n",
      "Repetition:  334\n",
      "Repetition:  335\n",
      "Repetition:  336\n",
      "Repetition:  337\n",
      "Repetition:  338\n",
      "Repetition:  339\n",
      "Repetition:  340\n",
      "Repetition:  341\n",
      "Repetition:  342\n",
      "Repetition:  343\n",
      "Repetition:  344\n",
      "Repetition:  345\n",
      "Repetition:  346\n",
      "Repetition:  347\n",
      "Repetition:  348\n",
      "Repetition:  349\n",
      "Repetition:  350\n",
      "Repetition:  351\n",
      "Elapsed: 01H 31m 46s\n",
      "Repetition:  352\n",
      "Repetition:  353\n",
      "Repetition:  354\n",
      "Repetition:  355\n",
      "Repetition:  356\n",
      "Repetition:  357\n",
      "Repetition:  358\n",
      "Repetition:  359\n",
      "Repetition:  360\n",
      "Repetition:  361\n",
      "Repetition:  362\n",
      "Repetition:  363\n",
      "Repetition:  364\n",
      "Repetition:  365\n",
      "Repetition:  366\n",
      "Repetition:  367\n",
      "Repetition:  368\n",
      "Repetition:  369\n",
      "Repetition:  370\n",
      "Repetition:  371\n",
      "Repetition:  372\n",
      "Repetition:  373\n",
      "Repetition:  374\n",
      "Repetition:  375\n",
      "Repetition:  376\n",
      "Repetition:  377\n",
      "Repetition:  378\n",
      "Repetition:  379\n",
      "Repetition:  380\n",
      "Repetition:  381\n",
      "Repetition:  382\n",
      "Repetition:  383\n",
      "Repetition:  384\n",
      "Repetition:  385\n",
      "Repetition:  386\n",
      "Repetition:  387\n",
      "Repetition:  388\n",
      "Repetition:  389\n",
      "Repetition:  390\n",
      "Repetition:  391\n",
      "Repetition:  392\n",
      "Repetition:  393\n",
      "Repetition:  394\n",
      "Repetition:  395\n",
      "Repetition:  396\n",
      "Repetition:  397\n",
      "Repetition:  398\n",
      "Repetition:  399\n",
      "Repetition:  400\n",
      "Repetition:  401\n",
      "Elapsed: 01H 44m 42s\n",
      "Repetition:  402\n",
      "Repetition:  403\n",
      "Repetition:  404\n",
      "Repetition:  405\n",
      "Repetition:  406\n",
      "Repetition:  407\n",
      "Repetition:  408\n",
      "Repetition:  409\n",
      "Repetition:  410\n",
      "Repetition:  411\n",
      "Repetition:  412\n",
      "Repetition:  413\n",
      "Repetition:  414\n",
      "Repetition:  415\n",
      "Repetition:  416\n",
      "Repetition:  417\n",
      "Repetition:  418\n",
      "Repetition:  419\n",
      "Repetition:  420\n",
      "Repetition:  421\n",
      "Repetition:  422\n",
      "Repetition:  423\n",
      "Repetition:  424\n",
      "Repetition:  425\n",
      "Repetition:  426\n",
      "Repetition:  427\n",
      "Repetition:  428\n",
      "Repetition:  429\n",
      "Repetition:  430\n",
      "Repetition:  431\n",
      "Repetition:  432\n",
      "Repetition:  433\n",
      "Repetition:  434\n",
      "Repetition:  435\n",
      "Repetition:  436\n",
      "Repetition:  437\n",
      "Repetition:  438\n",
      "Repetition:  439\n",
      "Repetition:  440\n",
      "Repetition:  441\n",
      "Repetition:  442\n",
      "Repetition:  443\n",
      "Repetition:  444\n",
      "Repetition:  445\n",
      "Repetition:  446\n",
      "Repetition:  447\n",
      "Repetition:  448\n",
      "Repetition:  449\n",
      "Repetition:  450\n",
      "Repetition:  451\n",
      "Elapsed: 01H 57m 37s\n",
      "Repetition:  452\n",
      "Repetition:  453\n",
      "Repetition:  454\n",
      "Repetition:  455\n",
      "Repetition:  456\n",
      "Repetition:  457\n",
      "Repetition:  458\n",
      "Repetition:  459\n",
      "Repetition:  460\n",
      "Repetition:  461\n",
      "Repetition:  462\n",
      "Repetition:  463\n",
      "Repetition:  464\n",
      "Repetition:  465\n",
      "Repetition:  466\n",
      "Repetition:  467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition:  468\n",
      "Repetition:  469\n",
      "Repetition:  470\n",
      "Repetition:  471\n",
      "Repetition:  472\n",
      "Repetition:  473\n",
      "Repetition:  474\n",
      "Repetition:  475\n",
      "Repetition:  476\n",
      "Repetition:  477\n",
      "Repetition:  478\n",
      "Repetition:  479\n",
      "Repetition:  480\n",
      "Repetition:  481\n",
      "Repetition:  482\n",
      "Repetition:  483\n",
      "Repetition:  484\n",
      "Repetition:  485\n",
      "Repetition:  486\n",
      "Repetition:  487\n",
      "Repetition:  488\n",
      "Repetition:  489\n",
      "Repetition:  490\n",
      "Repetition:  491\n",
      "Repetition:  492\n",
      "Repetition:  493\n",
      "Repetition:  494\n",
      "Repetition:  495\n",
      "Repetition:  496\n",
      "Repetition:  497\n",
      "Repetition:  498\n",
      "Repetition:  499\n",
      "Repetition:  500\n",
      "Repetition:  501\n",
      "Elapsed: 02H 10m 31s\n",
      "Repetition:  502\n",
      "Repetition:  503\n",
      "Repetition:  504\n",
      "Repetition:  505\n",
      "Repetition:  506\n",
      "Repetition:  507\n",
      "Repetition:  508\n",
      "Repetition:  509\n",
      "Repetition:  510\n",
      "Repetition:  511\n",
      "Repetition:  512\n",
      "Repetition:  513\n",
      "Repetition:  514\n",
      "Repetition:  515\n",
      "Repetition:  516\n",
      "Repetition:  517\n",
      "Repetition:  518\n",
      "Repetition:  519\n",
      "Repetition:  520\n",
      "Repetition:  521\n",
      "Repetition:  522\n",
      "Repetition:  523\n",
      "Repetition:  524\n",
      "Repetition:  525\n",
      "Repetition:  526\n",
      "Repetition:  527\n",
      "Repetition:  528\n",
      "Repetition:  529\n",
      "Repetition:  530\n",
      "Repetition:  531\n",
      "Repetition:  532\n",
      "Repetition:  533\n",
      "Repetition:  534\n",
      "Repetition:  535\n",
      "Repetition:  536\n",
      "Repetition:  537\n",
      "Repetition:  538\n",
      "Repetition:  539\n",
      "Repetition:  540\n",
      "Repetition:  541\n",
      "Repetition:  542\n",
      "Repetition:  543\n",
      "Repetition:  544\n",
      "Repetition:  545\n",
      "Repetition:  546\n",
      "Repetition:  547\n",
      "Repetition:  548\n",
      "Repetition:  549\n",
      "Repetition:  550\n",
      "Repetition:  551\n",
      "Elapsed: 02H 23m 27s\n",
      "Repetition:  552\n",
      "Repetition:  553\n",
      "Repetition:  554\n",
      "Repetition:  555\n",
      "Repetition:  556\n",
      "Repetition:  557\n",
      "Repetition:  558\n",
      "Repetition:  559\n",
      "Repetition:  560\n",
      "Repetition:  561\n",
      "Repetition:  562\n",
      "Repetition:  563\n",
      "Repetition:  564\n",
      "Repetition:  565\n",
      "Repetition:  566\n",
      "Repetition:  567\n",
      "Repetition:  568\n",
      "Repetition:  569\n",
      "Repetition:  570\n",
      "Repetition:  571\n",
      "Repetition:  572\n",
      "Repetition:  573\n",
      "Repetition:  574\n",
      "Repetition:  575\n",
      "Repetition:  576\n",
      "Repetition:  577\n",
      "Repetition:  578\n",
      "Repetition:  579\n",
      "Repetition:  580\n",
      "Repetition:  581\n",
      "Repetition:  582\n",
      "Repetition:  583\n",
      "Repetition:  584\n",
      "Repetition:  585\n",
      "Repetition:  586\n",
      "Repetition:  587\n",
      "Repetition:  588\n",
      "Repetition:  589\n",
      "Repetition:  590\n",
      "Repetition:  591\n",
      "Repetition:  592\n",
      "Repetition:  593\n",
      "Repetition:  594\n",
      "Repetition:  595\n",
      "Repetition:  596\n",
      "Repetition:  597\n",
      "Repetition:  598\n",
      "Repetition:  599\n",
      "Repetition:  600\n",
      "Repetition:  601\n",
      "Elapsed: 02H 36m 20s\n",
      "Repetition:  602\n",
      "Repetition:  603\n",
      "Repetition:  604\n",
      "Repetition:  605\n",
      "Repetition:  606\n",
      "Repetition:  607\n",
      "Repetition:  608\n",
      "Repetition:  609\n",
      "Repetition:  610\n",
      "Repetition:  611\n",
      "Repetition:  612\n",
      "Repetition:  613\n",
      "Repetition:  614\n",
      "Repetition:  615\n",
      "Repetition:  616\n",
      "Repetition:  617\n",
      "Repetition:  618\n",
      "Repetition:  619\n",
      "Repetition:  620\n",
      "Repetition:  621\n",
      "Repetition:  622\n",
      "Repetition:  623\n",
      "Repetition:  624\n",
      "Repetition:  625\n",
      "Repetition:  626\n",
      "Repetition:  627\n",
      "Repetition:  628\n",
      "Repetition:  629\n",
      "Repetition:  630\n",
      "Repetition:  631\n",
      "Repetition:  632\n",
      "Repetition:  633\n",
      "Repetition:  634\n",
      "Repetition:  635\n",
      "Repetition:  636\n",
      "Repetition:  637\n",
      "Repetition:  638\n",
      "Repetition:  639\n",
      "Repetition:  640\n",
      "Repetition:  641\n",
      "Repetition:  642\n",
      "Repetition:  643\n",
      "Repetition:  644\n",
      "Repetition:  645\n",
      "Repetition:  646\n",
      "Repetition:  647\n",
      "Repetition:  648\n",
      "Repetition:  649\n",
      "Repetition:  650\n",
      "Repetition:  651\n",
      "Elapsed: 02H 49m 16s\n",
      "Repetition:  652\n",
      "Repetition:  653\n",
      "Repetition:  654\n",
      "Repetition:  655\n",
      "Repetition:  656\n",
      "Repetition:  657\n",
      "Repetition:  658\n",
      "Repetition:  659\n",
      "Repetition:  660\n",
      "Repetition:  661\n",
      "Repetition:  662\n",
      "Repetition:  663\n",
      "Repetition:  664\n",
      "Repetition:  665\n",
      "Repetition:  666\n",
      "Repetition:  667\n",
      "Repetition:  668\n",
      "Repetition:  669\n",
      "Repetition:  670\n",
      "Repetition:  671\n",
      "Repetition:  672\n",
      "Repetition:  673\n",
      "Repetition:  674\n",
      "Repetition:  675\n",
      "Repetition:  676\n",
      "Repetition:  677\n",
      "Repetition:  678\n",
      "Repetition:  679\n",
      "Repetition:  680\n",
      "Repetition:  681\n",
      "Repetition:  682\n",
      "Repetition:  683\n",
      "Repetition:  684\n",
      "Repetition:  685\n",
      "Repetition:  686\n",
      "Repetition:  687\n",
      "Repetition:  688\n",
      "Repetition:  689\n",
      "Repetition:  690\n",
      "Repetition:  691\n",
      "Repetition:  692\n",
      "Repetition:  693\n",
      "Repetition:  694\n",
      "Repetition:  695\n",
      "Repetition:  696\n",
      "Repetition:  697\n",
      "Repetition:  698\n",
      "Repetition:  699\n",
      "Repetition:  700\n",
      "Repetition:  701\n",
      "Elapsed: 03H 02m 08s\n",
      "Repetition:  702\n",
      "Repetition:  703\n",
      "Repetition:  704\n",
      "Repetition:  705\n",
      "Repetition:  706\n",
      "Repetition:  707\n",
      "Repetition:  708\n",
      "Repetition:  709\n",
      "Repetition:  710\n",
      "Repetition:  711\n",
      "Repetition:  712\n",
      "Repetition:  713\n",
      "Repetition:  714\n",
      "Repetition:  715\n",
      "Repetition:  716\n",
      "Repetition:  717\n",
      "Repetition:  718\n",
      "Repetition:  719\n",
      "Repetition:  720\n",
      "Repetition:  721\n",
      "Repetition:  722\n",
      "Repetition:  723\n",
      "Repetition:  724\n",
      "Repetition:  725\n",
      "Repetition:  726\n",
      "Repetition:  727\n",
      "Repetition:  728\n",
      "Repetition:  729\n",
      "Repetition:  730\n",
      "Repetition:  731\n",
      "Repetition:  732\n",
      "Repetition:  733\n",
      "Repetition:  734\n",
      "Repetition:  735\n",
      "Repetition:  736\n",
      "Repetition:  737\n",
      "Repetition:  738\n",
      "Repetition:  739\n",
      "Repetition:  740\n",
      "Repetition:  741\n",
      "Repetition:  742\n",
      "Repetition:  743\n",
      "Repetition:  744\n",
      "Repetition:  745\n",
      "Repetition:  746\n",
      "Repetition:  747\n",
      "Repetition:  748\n",
      "Repetition:  749\n",
      "Repetition:  750\n",
      "Repetition:  751\n",
      "Elapsed: 03H 15m 04s\n",
      "Repetition:  752\n",
      "Repetition:  753\n",
      "Repetition:  754\n",
      "Repetition:  755\n",
      "Repetition:  756\n",
      "Repetition:  757\n",
      "Repetition:  758\n",
      "Repetition:  759\n",
      "Repetition:  760\n",
      "Repetition:  761\n",
      "Repetition:  762\n",
      "Repetition:  763\n",
      "Repetition:  764\n",
      "Repetition:  765\n",
      "Repetition:  766\n",
      "Repetition:  767\n",
      "Repetition:  768\n",
      "Repetition:  769\n",
      "Repetition:  770\n",
      "Repetition:  771\n",
      "Repetition:  772\n",
      "Repetition:  773\n",
      "Repetition:  774\n",
      "Repetition:  775\n",
      "Repetition:  776\n",
      "Repetition:  777\n",
      "Repetition:  778\n",
      "Repetition:  779\n",
      "Repetition:  780\n",
      "Repetition:  781\n",
      "Repetition:  782\n",
      "Repetition:  783\n",
      "Repetition:  784\n",
      "Repetition:  785\n",
      "Repetition:  786\n",
      "Repetition:  787\n",
      "Repetition:  788\n",
      "Repetition:  789\n",
      "Repetition:  790\n",
      "Repetition:  791\n",
      "Repetition:  792\n",
      "Repetition:  793\n",
      "Repetition:  794\n",
      "Repetition:  795\n",
      "Repetition:  796\n",
      "Repetition:  797\n",
      "Repetition:  798\n",
      "Repetition:  799\n",
      "Repetition:  800\n",
      "Repetition:  801\n",
      "Elapsed: 03H 27m 58s\n",
      "Repetition:  802\n",
      "Repetition:  803\n",
      "Repetition:  804\n",
      "Repetition:  805\n",
      "Repetition:  806\n",
      "Repetition:  807\n",
      "Repetition:  808\n",
      "Repetition:  809\n",
      "Repetition:  810\n",
      "Repetition:  811\n",
      "Repetition:  812\n",
      "Repetition:  813\n",
      "Repetition:  814\n",
      "Repetition:  815\n",
      "Repetition:  816\n",
      "Repetition:  817\n",
      "Repetition:  818\n",
      "Repetition:  819\n",
      "Repetition:  820\n",
      "Repetition:  821\n",
      "Repetition:  822\n",
      "Repetition:  823\n",
      "Repetition:  824\n",
      "Repetition:  825\n",
      "Repetition:  826\n",
      "Repetition:  827\n",
      "Repetition:  828\n",
      "Repetition:  829\n",
      "Repetition:  830\n",
      "Repetition:  831\n",
      "Repetition:  832\n",
      "Repetition:  833\n",
      "Repetition:  834\n",
      "Repetition:  835\n",
      "Repetition:  836\n",
      "Repetition:  837\n",
      "Repetition:  838\n",
      "Repetition:  839\n",
      "Repetition:  840\n",
      "Repetition:  841\n",
      "Repetition:  842\n",
      "Repetition:  843\n",
      "Repetition:  844\n",
      "Repetition:  845\n",
      "Repetition:  846\n",
      "Repetition:  847\n",
      "Repetition:  848\n",
      "Repetition:  849\n",
      "Repetition:  850\n",
      "Repetition:  851\n",
      "Elapsed: 03H 40m 52s\n",
      "Repetition:  852\n",
      "Repetition:  853\n",
      "Repetition:  854\n",
      "Repetition:  855\n",
      "Repetition:  856\n",
      "Repetition:  857\n",
      "Repetition:  858\n",
      "Repetition:  859\n",
      "Repetition:  860\n",
      "Repetition:  861\n",
      "Repetition:  862\n",
      "Repetition:  863\n",
      "Repetition:  864\n",
      "Repetition:  865\n",
      "Repetition:  866\n",
      "Repetition:  867\n",
      "Repetition:  868\n",
      "Repetition:  869\n",
      "Repetition:  870\n",
      "Repetition:  871\n",
      "Repetition:  872\n",
      "Repetition:  873\n",
      "Repetition:  874\n",
      "Repetition:  875\n",
      "Repetition:  876\n",
      "Repetition:  877\n",
      "Repetition:  878\n",
      "Repetition:  879\n",
      "Repetition:  880\n",
      "Repetition:  881\n",
      "Repetition:  882\n",
      "Repetition:  883\n",
      "Repetition:  884\n",
      "Repetition:  885\n",
      "Repetition:  886\n",
      "Repetition:  887\n",
      "Repetition:  888\n",
      "Repetition:  889\n",
      "Repetition:  890\n",
      "Repetition:  891\n",
      "Repetition:  892\n",
      "Repetition:  893\n",
      "Repetition:  894\n",
      "Repetition:  895\n",
      "Repetition:  896\n",
      "Repetition:  897\n",
      "Repetition:  898\n",
      "Repetition:  899\n",
      "Repetition:  900\n",
      "Repetition:  901\n",
      "Elapsed: 03H 53m 47s\n",
      "Repetition:  902\n",
      "Repetition:  903\n",
      "Repetition:  904\n",
      "Repetition:  905\n",
      "Repetition:  906\n",
      "Repetition:  907\n",
      "Repetition:  908\n",
      "Repetition:  909\n",
      "Repetition:  910\n",
      "Repetition:  911\n",
      "Repetition:  912\n",
      "Repetition:  913\n",
      "Repetition:  914\n",
      "Repetition:  915\n",
      "Repetition:  916\n",
      "Repetition:  917\n",
      "Repetition:  918\n",
      "Repetition:  919\n",
      "Repetition:  920\n",
      "Repetition:  921\n",
      "Repetition:  922\n",
      "Repetition:  923\n",
      "Repetition:  924\n",
      "Repetition:  925\n",
      "Repetition:  926\n",
      "Repetition:  927\n",
      "Repetition:  928\n",
      "Repetition:  929\n",
      "Repetition:  930\n",
      "Repetition:  931\n",
      "Repetition:  932\n",
      "Repetition:  933\n",
      "Repetition:  934\n",
      "Repetition:  935\n",
      "Repetition:  936\n",
      "Repetition:  937\n",
      "Repetition:  938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition:  939\n",
      "Repetition:  940\n",
      "Repetition:  941\n",
      "Repetition:  942\n",
      "Repetition:  943\n",
      "Repetition:  944\n",
      "Repetition:  945\n",
      "Repetition:  946\n",
      "Repetition:  947\n",
      "Repetition:  948\n",
      "Repetition:  949\n",
      "Repetition:  950\n",
      "Repetition:  951\n",
      "Elapsed: 04H 06m 39s\n",
      "Repetition:  952\n",
      "Repetition:  953\n",
      "Repetition:  954\n",
      "Repetition:  955\n",
      "Repetition:  956\n",
      "Repetition:  957\n",
      "Repetition:  958\n",
      "Repetition:  959\n",
      "Repetition:  960\n",
      "Repetition:  961\n",
      "Repetition:  962\n",
      "Repetition:  963\n",
      "Repetition:  964\n",
      "Repetition:  965\n",
      "Repetition:  966\n",
      "Repetition:  967\n",
      "Repetition:  968\n",
      "Repetition:  969\n",
      "Repetition:  970\n",
      "Repetition:  971\n",
      "Repetition:  972\n",
      "Repetition:  973\n",
      "Repetition:  974\n",
      "Repetition:  975\n",
      "Repetition:  976\n",
      "Repetition:  977\n",
      "Repetition:  978\n",
      "Repetition:  979\n",
      "Repetition:  980\n",
      "Repetition:  981\n",
      "Repetition:  982\n",
      "Repetition:  983\n",
      "Repetition:  984\n",
      "Repetition:  985\n",
      "Repetition:  986\n",
      "Repetition:  987\n",
      "Repetition:  988\n",
      "Repetition:  989\n",
      "Repetition:  990\n",
      "Repetition:  991\n",
      "Repetition:  992\n",
      "Repetition:  993\n",
      "Repetition:  994\n",
      "Repetition:  995\n",
      "Repetition:  996\n",
      "Repetition:  997\n",
      "Repetition:  998\n",
      "Repetition:  999\n",
      "Repetition:  1000\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "timer_start = time.time()\n",
    "print('Simulation start: %s' %time.ctime(int(timer_start)))\n",
    "\n",
    "# run specified number of repetitions\n",
    "results = []\n",
    "preds = []\n",
    "for i in range(reps):\n",
    "    \n",
    "    # print repetition\n",
    "    print('Repetition: ',i+1)\n",
    "    \n",
    "    # train on GPU\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # loop over all break locations\n",
    "    df_sim = pd.DataFrame()\n",
    "    for j in range(len(tau)):\n",
    "        \n",
    "        # extract data\n",
    "        X_train = data[j][0]\n",
    "        X_test = data[j][1]\n",
    "        y_train = data[j][2]\n",
    "        y_test = data[j][3]\n",
    "        \n",
    "        # convert data to tensors\n",
    "        train_features = torch.Tensor(X_train)\n",
    "        train_targets = torch.Tensor(y_train)\n",
    "        test_features = torch.Tensor(X_test)\n",
    "        test_targets = torch.Tensor(y_test)\n",
    "        \n",
    "        # build tensor dataset\n",
    "        train = TensorDataset(train_features, train_targets)\n",
    "        test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "        # get batched data\n",
    "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=False) \n",
    "        #test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n",
    "        \n",
    "        # initialise model\n",
    "        torch.manual_seed(i)\n",
    "        model = get_model(model_name, model_params).to(device)\n",
    "        \n",
    "        # create loss function and optimizer\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "        \n",
    "        # train the model\n",
    "        model_out = opt.train(train_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=1)\n",
    "            \n",
    "        # evaluate on test set\n",
    "        predictions, values = opt.evaluate(model_out, test_loader_one, batch_size=1, n_features=1)\n",
    "        if j == 0:\n",
    "            pred_bl = np.expand_dims(np.squeeze(predictions),axis=0)\n",
    "        else:\n",
    "            pred_bl = np.concatenate((pred_bl,np.expand_dims(np.squeeze(predictions),axis=0)),axis=0)\n",
    "        df_result = format_predictions_dl(predictions, values)\n",
    "        result_metrics = calculate_metrics(df_result)\n",
    "        \n",
    "        # append metrics on test set\n",
    "        df_metrics = pd.DataFrame(np.expand_dims((result_metrics['rmse'],result_metrics['mae'],result_metrics['mape'],result_metrics['r2'],),axis=0),columns=['mse','mae','mape','r2'])\n",
    "        df_sim = pd.concat([df_sim,df_metrics],axis=0, ignore_index=True)\n",
    "    \n",
    "    # save results of every repetition\n",
    "    results.append(df_sim) \n",
    "    preds.append(pred_bl)\n",
    "    \n",
    "    if (i < 10) | (i % 50 == 0):\n",
    "        print('Elapsed: %s' %time_format(time.time() - timer_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13250a91",
   "metadata": {},
   "source": [
    "Evaluate and save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa58f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_results = np.asarray(results) # dims: no. of reps x no. of break locations x no. of metrics\n",
    "arr_preds = np.asarray(preds) # dims: no. of reps x no. of break locations x no. of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "615ba9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "arr_mean = np.mean(arr_results,axis=0)\n",
    "arr_std = np.std(arr_results,axis=0)\n",
    "arr_min = np.min(arr_results,axis=0)\n",
    "arr_max = np.max(arr_results,axis=0)\n",
    "arr_median = np.median(arr_results,axis=0)\n",
    "\n",
    "# predictions\n",
    "arr_mean_pred = np.mean(arr_preds,axis=0)\n",
    "arr_std_pred = np.std(arr_preds,axis=0)\n",
    "arr_min_pred = np.min(arr_preds,axis=0)\n",
    "arr_max_pred = np.max(arr_preds,axis=0)\n",
    "arr_median_pred = np.median(arr_preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e47c575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(path+'Plot_'+model_name+'_results.npz',mean=arr_mean,std=arr_std,minimum=arr_min,maximum=arr_max,median=arr_median)\n",
    "np.savez(path+'Plot_'+model_name+'_preds.npz',mean=arr_mean_pred,std=arr_std_pred,minimum=arr_min_pred,maximum=arr_max_pred,median=arr_median_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c58a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
