{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0de3307",
   "metadata": {},
   "source": [
    "# Simulation: Dependence on Break Locations with Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8bbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017af60",
   "metadata": {},
   "source": [
    "Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb283dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Meier/Dropbox (Institut f√ºr Statistik)/Structural Breaks + DL/Figures/Results/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438102cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RNN' # 'RNN', 'LSTM', 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 1000           # number of repetitions\n",
    "sim_length = 500      # length of simulated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1           # proportion of test set\n",
    "lags = 1                  # number of lags as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e70c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_type = 'tukey-hanning' # 'exponential', 'rayleigh', 'bartlett', 'parzen' , 'tukey-hanning'\n",
    "if weight_type == 'bartlett' or weight_type == 'parzen' or weight_type == 'tukey-hanning':\n",
    "    fac = 1 # {1, 0.95, 0.9}\n",
    "    alpha = (sim_length*(1-test_size))**fac   # kernels: {T, T^0.95, T^0.9}\n",
    "else:\n",
    "    alpha = 0.005 # exp: {0.005, 0.01, 0.02}, ray: {2*10**(-5), 5*10**(-5), 1*10**(-4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "input_dim = lags                  # number of lagged features in X\n",
    "hidden_dim = 10                   # number of hidden nodes per layer\n",
    "layer_dim = 1                     # number of layers\n",
    "output_dim = 1                    # output dimension (1 for univariate output)\n",
    "dropout = 0                       # dropout proportion (only before the last sequential layer)\n",
    "learning_rate = 1e-3              # learning rate for Adam optimizer\n",
    "weight_decay = 1e-6               # weight decay for Adam optimizer\n",
    "\n",
    "# save model parameters in dict\n",
    "model_params = {'input_dim': input_dim, 'hidden_dim' : hidden_dim,'layer_dim' : layer_dim, 'output_dim' : output_dim, 'dropout_prob' : dropout}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82bcfc",
   "metadata": {},
   "source": [
    "Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        self.rnn.flatten_parameters() # ------------------------------------------------------------------\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        self.lstm.flatten_parameters() # ------------------------------------------------------------------\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        self.gru.flatten_parameters() # ------------------------------------------------------------------\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6523ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"rnn\": RNNModel,\n",
    "        \"lstm\": LSTMModel,\n",
    "        \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "    \n",
    "    def train_step(self, x, y, weight):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat, weight)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, train_loader, weight_type, alpha, batch_size=64, n_epochs=50, n_features=1):\n",
    "        \n",
    "        # train on GPU\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            obs_total = len(train_loader.dataset)  # number of samples\n",
    "            step = 1\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([len(y_batch), -1, n_features]).to(device)\n",
    "                y_batch = y_batch.view([len(y_batch), -1]).to(device)\n",
    "                t = np.arange(step,step+len(y_batch)) # array of periods of this batch\n",
    "                if weight_type == 'exponential':\n",
    "                    weight = np.exp(-alpha*(obs_total-t))  # exponential weight\n",
    "                if weight_type == 'rayleigh':\n",
    "                    weight = np.exp(-(1/2)*alpha*(obs_total-t)**2)  # Rayleigh weight\n",
    "                if weight_type == 'bartlett':  # Bartlett weight\n",
    "                    weight = np.zeros([t.shape[0]])\n",
    "                    for i in range(t.shape[0]):\n",
    "                        if ((obs_total-t[i])/alpha)<=1:\n",
    "                            weight[i] = 1-(obs_total-t[i])/alpha\n",
    "                        else:\n",
    "                            weight[i] = 0\n",
    "                if weight_type == 'parzen':  # Parzen weight\n",
    "                    weight = np.zeros([t.shape[0]])\n",
    "                    for i in range(t.shape[0]):\n",
    "                        if ((obs_total-t[i])/alpha)>=0 and ((obs_total-t[i])/alpha)<=0.5:\n",
    "                            weight[i] = (1-6*((obs_total-t[i])/alpha)**2+6*((obs_total-t[i])/alpha)**3) \n",
    "                        elif ((obs_total-t[i])/alpha)>0.5 and ((obs_total-t[i])/alpha)<=1:\n",
    "                            weight[i] = 2*(1-((obs_total-t[i])/alpha))**3 \n",
    "                        else:\n",
    "                            weight[i] = 0\n",
    "                if weight_type == 'tukey-hanning':  # Tukey-Hanning weight\n",
    "                    weight = np.zeros([t.shape[0]])\n",
    "                    for i in range(t.shape[0]):\n",
    "                        if ((obs_total-t[i])/alpha)<=1:\n",
    "                            weight[i] = (1+np.cos(np.pi*((obs_total-t[i])/alpha)))/2\n",
    "                        else:\n",
    "                            weight[i] = 0\n",
    "                b_loss = self.train_step(x_batch, y_batch, torch.tensor(weight).to(device))\n",
    "                batch_losses.append(b_loss)\n",
    "                step += len(y_batch) ###########################################\n",
    "            training_loss = np.sum(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, best_model, test_loader, batch_size=1, n_features=1):\n",
    "        # evaluate on GPU\n",
    "        device = torch.device('cuda')\n",
    "        model = deepcopy(best_model)\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.view([batch_size, -1]).to(device)\n",
    "                model.eval()\n",
    "                yhat = model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().cpu().numpy())\n",
    "                values.append(y_test.to(device).detach().cpu().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_dl(predictions, values):\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds})\n",
    "    df_result = df_result.sort_index()\n",
    "    return df_result\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    return {'rmse' : mean_squared_error(df.value, df.prediction)**0.5,\n",
    "            'mae' : mean_absolute_error(df.value, df.prediction),\n",
    "            'mape': mean_absolute_percentage_error(df.value, df.prediction),\n",
    "            'r2' : r2_score(df.value, df.prediction)}\n",
    "\n",
    "def time_format(seconds: int):\n",
    "    if seconds is not None:\n",
    "        seconds = int(seconds)\n",
    "        d = seconds // (3600 * 24)\n",
    "        h = seconds // 3600 % 24\n",
    "        m = seconds % 3600 // 60\n",
    "        s = seconds % 3600 % 60\n",
    "        if d > 0:\n",
    "            return '{:02d}D {:02d}H {:02d}m {:02d}s'.format(d, h, m, s)\n",
    "        elif h > 0:\n",
    "            return '{:02d}H {:02d}m {:02d}s'.format(h, m, s)\n",
    "        elif m > 0:\n",
    "            return '{:02d}m {:02d}s'.format(m, s)\n",
    "        elif s > 0:\n",
    "            return '{:02d}s'.format(s)\n",
    "    return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedLoss, self).__init__()\n",
    " \n",
    "    def forward(self, inputs, targets, weight=1):  \n",
    "        \n",
    "        # flatten input and target tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # get weighted squared differences\n",
    "        weighted_sqrd_diff = weight*((inputs - targets) ** 2)\n",
    "        \n",
    "        return weighted_sqrd_diff.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c9e0d",
   "metadata": {},
   "source": [
    "Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over break locations\n",
    "tau = np.arange(0,1,0.05)\n",
    "data = []\n",
    "for j in range(len(tau)):\n",
    "        \n",
    "    # simulate constant data (with mean break from 0.5 to 0)\n",
    "    if j == 0:\n",
    "        data_temp = 0.5*np.ones(int(sim_length)+lags)\n",
    "    else:\n",
    "        data_temp = np.concatenate((np.zeros(int(np.round((1-test_size)*sim_length*tau[j],0)+lags)),0.5*np.ones(int(np.round((1-test_size)*sim_length*(1-tau[j])+test_size*sim_length,0)))),axis=0)   \n",
    "\n",
    "    # split into training and test data\n",
    "    X_temp = data_temp[:-1]\n",
    "    y_temp = data_temp[1:]\n",
    "    data.append(train_test_split(X_temp, y_temp, test_size=test_size, shuffle=False)) # X_train, X_test, y_train, y_test for every j in tau "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802eefef",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "timer_start = time.time()\n",
    "print('Simulation start: %s' %time.ctime(int(timer_start)))\n",
    "\n",
    "# run specified number of repetitions\n",
    "results = []\n",
    "preds = []\n",
    "for i in range(reps):\n",
    "    \n",
    "    # print repetition\n",
    "    print('Repetition: ',i+1)\n",
    "    \n",
    "    # train on GPU\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # loop over all break locations\n",
    "    df_sim = pd.DataFrame()\n",
    "    for j in range(len(tau)):\n",
    "        \n",
    "        # extract data\n",
    "        X_train = data[j][0]\n",
    "        X_test = data[j][1]\n",
    "        y_train = data[j][2]\n",
    "        y_test = data[j][3]\n",
    "        \n",
    "        # convert data to tensors\n",
    "        train_features = torch.Tensor(X_train)\n",
    "        train_targets = torch.Tensor(y_train)\n",
    "        test_features = torch.Tensor(X_test)\n",
    "        test_targets = torch.Tensor(y_test)\n",
    "        \n",
    "        # build tensor dataset\n",
    "        train = TensorDataset(train_features, train_targets)\n",
    "        test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "        # get batched data\n",
    "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=False) \n",
    "        test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n",
    "        \n",
    "        # initialise model\n",
    "        torch.manual_seed(i)\n",
    "        model = get_model(model_name, model_params).to(device)\n",
    "        \n",
    "        # create loss function and optimizer\n",
    "        loss_fn = WeightedLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "        \n",
    "        # train the model\n",
    "        model_out = opt.train(train_loader, weight_type=weight_type, alpha=alpha, batch_size=batch_size, n_epochs=n_epochs, n_features=1)\n",
    "            \n",
    "        # evaluate on test set\n",
    "        predictions, values = opt.evaluate(model_out, test_loader_one, batch_size=1, n_features=1)\n",
    "        if j == 0:\n",
    "            pred_bl = np.expand_dims(np.squeeze(predictions),axis=0)\n",
    "        else:\n",
    "            pred_bl = np.concatenate((pred_bl,np.expand_dims(np.squeeze(predictions),axis=0)),axis=0)\n",
    "        df_result = format_predictions_dl(predictions, values)\n",
    "        result_metrics = calculate_metrics(df_result)\n",
    "        \n",
    "        # append metrics on test set\n",
    "        df_metrics = pd.DataFrame(np.expand_dims((result_metrics['rmse'],result_metrics['mae'],result_metrics['mape'],result_metrics['r2'],),axis=0),columns=['mse','mae','mape','r2'])\n",
    "        df_sim = pd.concat([df_sim,df_metrics],axis=0, ignore_index=True)\n",
    "    \n",
    "    # save results of every repetition\n",
    "    results.append(df_sim) \n",
    "    preds.append(pred_bl)\n",
    "    \n",
    "    if (i < 10) | (i % 50 == 0):\n",
    "        print('Elapsed: %s' %time_format(time.time() - timer_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13250a91",
   "metadata": {},
   "source": [
    "Evaluate and save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa58f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_results = np.asarray(results) # dims: no. of reps x no. of break locations x no. of metrics\n",
    "arr_preds = np.asarray(preds) # dims: no. of reps x no. of break locations x no. of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ba9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "arr_mean = np.mean(arr_results,axis=0)\n",
    "arr_std = np.std(arr_results,axis=0)\n",
    "arr_min = np.min(arr_results,axis=0)\n",
    "arr_max = np.max(arr_results,axis=0)\n",
    "arr_median = np.median(arr_results,axis=0)\n",
    "\n",
    "# predictions\n",
    "arr_mean_pred = np.mean(arr_preds,axis=0)\n",
    "arr_std_pred = np.std(arr_preds,axis=0)\n",
    "arr_min_pred = np.min(arr_preds,axis=0)\n",
    "arr_max_pred = np.max(arr_preds,axis=0)\n",
    "arr_median_pred = np.median(arr_preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if weight_type == 'bartlett' or weight_type == 'parzen' or weight_type == 'tukey-hanning':\n",
    "    np.savez(path+'Plot_'+model_name+'_'+weight_type+'_T'+str(fac)+'_results.npz',mean=arr_mean,std=arr_std,minimum=arr_min,maximum=arr_max,median=arr_median)\n",
    "    np.savez(path+'Plot_'+model_name+'_'+weight_type+'_T'+str(fac)+'_preds.npz',mean=arr_mean_pred,std=arr_std_pred,minimum=arr_min_pred,maximum=arr_max_pred,median=arr_median_pred)\n",
    "else:\n",
    "    np.savez(path+'Plot_'+model_name+'_'+weight_type+'_'+str(alpha)+'_results.npz',mean=arr_mean,std=arr_std,minimum=arr_min,maximum=arr_max,median=arr_median)\n",
    "    np.savez(path+'Plot_'+model_name+'_'+weight_type+'_'+str(alpha)+'_preds.npz',mean=arr_mean_pred,std=arr_std_pred,minimum=arr_min_pred,maximum=arr_max_pred,median=arr_median_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
