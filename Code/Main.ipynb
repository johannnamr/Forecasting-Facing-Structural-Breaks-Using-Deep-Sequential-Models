{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4decb21e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f630e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265c06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Meier/Dropbox (Institut für Statistik)/Structural Breaks + DL/Simulation/Python Code/' \n",
    "# path = 'C:/Users/Johan/Desktop/Local/'\n",
    "#path= 'C:/Users/Johan/Dropbox (Institut für Statistik)/Structural Breaks + DL/Simulation/Python Code/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ba88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_model = 'RNN' # 'RNN', 'LSTM', 'GRU', 'AR', 'ARIMA', 'GARCH'\n",
    "setting = 'AR' # 'AR', 'ARIMA', 'GARCH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e0c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 1000         # number of repetitions\n",
    "ave = True       # return average predictions for sequential DL models\n",
    "no_sdl = 10       # number of sequential DL models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9806bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1                                # proportion of test set\n",
    "sim_length = 5000                              # length of simulated sample\n",
    "tau = np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7])  # break locations\n",
    "fac_beta = np.array([0.5,1,2])                 # break size factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4dafb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1           # proportion of test set\n",
    "val_size = 0.2            # proportion of validation set\n",
    "lags = 1                  # number of lags as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817196f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if setting == 'AR':\n",
    "    params = np.array([0.1,0.5,0.9,0.95,0.99]) # AR (phi)\n",
    "if setting == 'ARIMA':\n",
    "    params = [np.array([0.1,0.5,0.9,0.95,0.99]),np.array([0.3,-0.3])] # ARIMA (phi, theta)\n",
    "    params_list = list(itertools.product(params[0],params[1]))\n",
    "    print(params_list)\n",
    "if setting == 'GARCH':\n",
    "    ! pip install arch\n",
    "    params = [np.array([0.1,0.8]), np.array([0.45,0.45]), np.array([0.8,0.1])] # GARCH (phi,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe0266",
   "metadata": {},
   "source": [
    "Run external notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c01fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Helper_functions.ipynb\" # notebook containing helper functions\n",
    "%run \"Simulate_data.ipynb\" # notebook containing simulation function\n",
    "%run \"Split_data.ipynb\" # notebook containing split function\n",
    "%run \"DL_models.ipynb\" # notebook containing sequential deep learning models\n",
    "%run \"AR_model.ipynb\" # notebook containing AR model\n",
    "%run \"ARIMA_model.ipynb\" # notebook containing AR model\n",
    "%run \"GARCH_model.ipynb\" # notebook containing AR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc876a",
   "metadata": {},
   "source": [
    "Get names of simulation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fddfe5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sims = get_str_sims(tau, fac_beta) # get names of simulation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beab80",
   "metadata": {},
   "source": [
    "Run simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83efb659",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation start: Wed Jul 27 08:04:27 2022\n",
      "Repetition:  1\n",
      "Parameter combination:  1 / 5\n",
      "No. of dataset:  1 / 9\n",
      "No. of dataset:  2 / 9\n",
      "No. of dataset:  3 / 9\n",
      "No. of dataset:  4 / 9\n",
      "No. of dataset:  5 / 9\n",
      "No. of dataset:  6 / 9\n",
      "No. of dataset:  7 / 9\n",
      "No. of dataset:  8 / 9\n",
      "No. of dataset:  9 / 9\n",
      "Parameter combination:  2 / 5\n",
      "No. of dataset:  1 / 9\n",
      "No. of dataset:  2 / 9\n",
      "No. of dataset:  3 / 9\n",
      "No. of dataset:  4 / 9\n",
      "No. of dataset:  5 / 9\n",
      "No. of dataset:  6 / 9\n",
      "No. of dataset:  7 / 9\n",
      "No. of dataset:  8 / 9\n",
      "No. of dataset:  9 / 9\n",
      "(2, 1, 9, 4)\n",
      "(2, 1, 9, 4)\n",
      "(1, 9, 4) (1, 9, 4) (1, 9, 4) (1, 9, 4) (1, 9, 4)\n",
      "Elapsed: 01H 36m 41s\n",
      "Repetition:  2\n",
      "Parameter combination:  1 / 5\n",
      "No. of dataset:  1 / 9\n",
      "No. of dataset:  2 / 9\n",
      "No. of dataset:  3 / 9\n",
      "No. of dataset:  4 / 9\n",
      "No. of dataset:  5 / 9\n",
      "No. of dataset:  6 / 9\n",
      "No. of dataset:  7 / 9\n",
      "No. of dataset:  8 / 9\n",
      "No. of dataset:  9 / 9\n",
      "Parameter combination:  2 / 5\n",
      "No. of dataset:  1 / 9\n",
      "No. of dataset:  2 / 9\n",
      "No. of dataset:  3 / 9\n",
      "No. of dataset:  4 / 9\n",
      "No. of dataset:  5 / 9\n",
      "No. of dataset:  6 / 9\n",
      "No. of dataset:  7 / 9\n",
      "No. of dataset:  8 / 9\n",
      "No. of dataset:  9 / 9\n",
      "(2, 1, 9, 4)\n",
      "(2, 2, 9, 4)\n",
      "(2, 2, 9, 4)\n",
      "(2, 9, 4) (2, 9, 4) (2, 9, 4) (2, 9, 4) (2, 9, 4)\n",
      "Elapsed: 03H 18m 39s\n",
      "Simulation end: Wed Jul 27 11:23:06 2022\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "timer_start = time.time()\n",
    "print('Simulation start: %s' %time.ctime(int(timer_start)))\n",
    "\n",
    "# delete all files in Temp folder\n",
    "emtpy_temp(path+'Temp/')\n",
    "\n",
    "# run specified number of repetitions\n",
    "for i in range(reps):\n",
    "    \n",
    "    # print repetition\n",
    "    print('Repetition: ',i+1)\n",
    "    \n",
    "    # seed\n",
    "    np.random.seed(i)\n",
    "    \n",
    "    # delete all simulation files in Temp folder\n",
    "    del_sim(path+'Temp/')\n",
    "    \n",
    "    # simulate data for given setting and parameters(save csv-files in Temp)\n",
    "    sim_data(setting=setting, params=params, tau=tau, fac_beta=fac_beta, test_size=test_size, l=sim_length, path=path+'Temp/', lags=1, verbose=False)\n",
    "    \n",
    "    # split all data into train, val, and test (save nzp-files in Temp)\n",
    "    split_data(setting=setting, params=params, path=path+'Temp/', test_size=test_size, val_size=val_size)\n",
    "    \n",
    "    # check simulated data\n",
    "    #sim = pd.read_csv(path+'Temp/sim1.csv' ,sep=',',na_values = 'NA')\n",
    "    #data1 = np.load(path+'Temp/sim2_%s.npz' %sim.columns[0])\n",
    "    #data2 = np.load(path+'Temp/sim2_%s.npz' %sim.columns[1])\n",
    "    #plt.plot(data1['y_test'])\n",
    "    #plt.plot(data2['y_test'])\n",
    "    #plt.show()\n",
    "    #del data1,data2\n",
    "             \n",
    "    # DL forecast\n",
    "    # if DL: get combination forecasts\n",
    "    if str_model == 'RNN' or str_model =='LSTM' or str_model =='GRU':\n",
    "        \n",
    "        #%run \"DL_models.ipynb\" # notebook containing sequential deep learning models\n",
    "        \n",
    "        # random seed for each model\n",
    "        seed = np.random.randint(low=1,high=10000000,size=no_sdl) # number of models to run per dataset(random seeds)\n",
    "        \n",
    "        # set parameters\n",
    "        input_dim = lags                  # number of lagged features in X\n",
    "        hidden_dim = 10                   # number of hidden nodes per layer\n",
    "        layer_dim = 1                     # number of layers\n",
    "        output_dim = 1                    # output dimension (1 for univariate output)\n",
    "        dropout = 0                       # dropout proportion (only before the last sequential layer)\n",
    "        learning_rate = 1e-3              # learning rate for Adam optimizer\n",
    "        weight_decay = 1e-6               # weight decay for Adam optimizer\n",
    "\n",
    "        # save model parameters in dict\n",
    "        model_params = {'input_dim': input_dim, 'hidden_dim' : hidden_dim,'layer_dim' : layer_dim, 'output_dim' : output_dim, 'dropout_prob' : dropout}\n",
    "        \n",
    "        # train model\n",
    "        results = []\n",
    "        ave_results = []\n",
    "        for j in range(len(params)):\n",
    "        #for j in range(2):\n",
    "            print('Parameter combination: ', j+1,'/',len(params))\n",
    "            results_seeds, sim_preds, best_seeds = train_loop(model_name=str_model,model_params=model_params,num_sim=j+1,str_sims=str_sims,path=path,seed=seed)\n",
    "            results.append(np.squeeze(results_seeds[[np.arange(0,len(str_sims))],list(map(int, best_seeds)),:])) # shape: 1 x no. of settings x no. of metrics\n",
    "        \n",
    "            if ave:\n",
    "                ave_results_sets = []\n",
    "                for l in range(len(str_sims)):\n",
    "                    ave_preds = np.mean(sim_preds,axis=1)[l,:,:]\n",
    "                    df_ave = pd.DataFrame(data={\"value\": ave_preds[:,0], \"prediction\": ave_preds[:,1]})\n",
    "                    ave_result_metrics = calculate_metrics(df_ave)\n",
    "                    df_ave_metrics = pd.DataFrame(np.expand_dims((ave_result_metrics['rmse'],ave_result_metrics['mae'],ave_result_metrics['mape'],ave_result_metrics['r2'],),axis=0),columns=['rmse','mae','mape','r2'])\n",
    "                    ave_results_sets.append(df_ave_metrics)\n",
    "                ave_results_sets = np.expand_dims(np.squeeze(np.asarray(ave_results_sets)),axis=0) # shape: 1 x no. of settings x no. of metrics\n",
    "                ave_results.append(ave_results_sets)\n",
    "    \n",
    "    # forecast AR model\n",
    "    if str_model == 'AR':\n",
    "        \n",
    "        results = []\n",
    "        for j in range(len(params)):\n",
    "        #for j in range(2):\n",
    "            print('Parameter combination: ', j+1,'/',len(params))\n",
    "            df_sim = train_loop_ar(path=path,num_sim=j+1,str_sims=str_sims)\n",
    "            results.append(df_sim)\n",
    "    \n",
    "    # forecast ARIMA model\n",
    "    if str_model == 'ARIMA':\n",
    "        \n",
    "        params_list = list(itertools.product(params[0],params[1]))\n",
    "        \n",
    "        results = []\n",
    "        for j in range(len(params_list)):\n",
    "        #for j in range(2):\n",
    "            print('Parameter combination: ', j+1,'/',len(params_list))\n",
    "            df_sim = train_loop_arima(path=path,num_sim=j+1,str_sims=str_sims)\n",
    "            results.append(df_sim)\n",
    "    \n",
    "    # forecast GARCH model\n",
    "    if str_model == 'GARCH':\n",
    "        \n",
    "        results = []\n",
    "        for j in range(len(params)):\n",
    "        #for j in range(2):\n",
    "            print('Parameter combination: ', j+1,'/',len(params))\n",
    "            df_sim = train_loop_garch(path=path,num_sim=j+1,str_sims=str_sims)\n",
    "            results.append(df_sim)\n",
    "        \n",
    "    \n",
    "    # save intermediate results\n",
    "    new_results = np.expand_dims(np.asarray(results),axis=1)\n",
    "    if i==0:\n",
    "        np.save(path+'Temp/interm_results.npy',new_results)\n",
    "    else:\n",
    "        prev_results = np.load(path+'Temp/interm_results.npy')\n",
    "        all_results = np.concatenate((prev_results,new_results),axis=1) # shape: no. of params x rep x no. of settings x no. of metrics\n",
    "        np.save(path+'Temp/interm_results.npy',all_results)\n",
    "        del prev_results\n",
    "    \n",
    "    # calculate metrics\n",
    "    arr_results = np.load(path+'Temp/interm_results.npy')\n",
    "    arr_mean, arr_std, arr_min, arr_max, arr_median = get_results(arr_results) # aggregate over reps (axis 1)  \n",
    "    np.savez(path+'Results/'+setting+'_'+str_model+'_results.npz',mean=arr_mean,std=arr_std,minimum=arr_min,maximum=arr_max,median=arr_median)\n",
    "    del arr_results\n",
    "    \n",
    "    # if predictions averaging is required\n",
    "    if ave and (str_model == 'RNN' or str_model =='LSTM' or str_model =='GRU'):\n",
    "        \n",
    "        # save intermediate results\n",
    "        new_ave_results = np.asarray(ave_results)\n",
    "        if i==0:\n",
    "            np.save(path+'Temp/interm_ave_results.npy',new_ave_results)\n",
    "        else:\n",
    "            prev_ave_results = np.load(path+'Temp/interm_ave_results.npy')\n",
    "            all_ave_results = np.concatenate((prev_ave_results,new_ave_results),axis=1) # shape: no. of params x rep x no. of settings x no. of metrics\n",
    "            np.save(path+'Temp/interm_ave_results.npy',all_ave_results)\n",
    "            del prev_ave_results\n",
    "    \n",
    "        # calculate metrics\n",
    "        arr_ave_results = np.load(path+'Temp/interm_ave_results.npy')\n",
    "        arr_mean, arr_std, arr_min, arr_max, arr_median = get_results(arr_ave_results) # aggregate over reps (axis 1)  \n",
    "        np.savez(path+'Results/'+setting+'_'+str_model+'_ave_results.npz',mean=arr_mean,std=arr_std,minimum=arr_min,maximum=arr_max,median=arr_median) # shape: no. of parameters x no. of settings x no. of metrics\n",
    "        del arr_ave_results\n",
    "    \n",
    "    if (i < 10) | (i % 50 == 0):\n",
    "        print('Elapsed: %s' %time_format(time.time() - timer_start))\n",
    "\n",
    "print('Simulation end: %s' %time.ctime(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7baf006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.99020052 0.79739594 2.21365619 0.13579388]\n",
      "  [0.99007449 0.79726627 2.22717369 0.13606413]\n",
      "  [0.98820113 0.79596564 2.21812564 0.13952391]\n",
      "  [0.9871878  0.79457909 2.31536847 0.14138716]\n",
      "  [0.98917747 0.79742649 2.18271893 0.13878147]\n",
      "  [0.98957937 0.79691237 2.23571348 0.13701659]\n",
      "  [0.98651938 0.79483235 2.17000651 0.14287008]\n",
      "  [0.98911013 0.79722783 2.29028457 0.1386913 ]\n",
      "  [0.98944224 0.79665351 2.19767529 0.13787704]]\n",
      "\n",
      " [[0.97448882 0.77168274 3.61348051 0.14021159]\n",
      "  [0.974397   0.77172256 3.63116562 0.14044712]\n",
      "  [0.97618454 0.77367303 3.62402135 0.13700554]\n",
      "  [0.97224579 0.77065495 3.93686593 0.14387428]\n",
      "  [0.97270912 0.77162951 4.05365837 0.14294248]\n",
      "  [0.97487431 0.77200225 3.54940224 0.13974162]\n",
      "  [0.97465634 0.77152598 3.52071697 0.13978017]\n",
      "  [0.97184184 0.77101457 4.24833047 0.14455148]\n",
      "  [0.97144308 0.77014136 4.14704412 0.14540709]]] [[[6.97308364e-03 8.17865133e-03 7.49534369e-01 1.42930159e-01]\n",
      "  [7.13916609e-03 8.27178359e-03 7.65524507e-01 1.42602072e-01]\n",
      "  [7.77177152e-03 8.71565938e-03 8.06569517e-01 1.40936114e-01]\n",
      "  [8.09705171e-03 9.51808691e-03 8.47413242e-01 1.40066720e-01]\n",
      "  [1.10141402e-02 1.08000934e-02 9.33268845e-01 1.35570033e-01]\n",
      "  [7.43082912e-03 8.49282742e-03 7.85529613e-01 1.41943846e-01]\n",
      "  [9.17130706e-03 9.68503952e-03 8.54440689e-01 1.37997636e-01]\n",
      "  [1.03080450e-02 1.06445849e-02 9.63831961e-01 1.36782062e-01]\n",
      "  [9.51386628e-03 1.00705028e-02 7.60608613e-01 1.38266094e-01]]\n",
      "\n",
      " [[5.31028733e-03 1.29836798e-03 2.29760462e+00 1.19492368e-01]\n",
      "  [4.99721094e-03 1.56772137e-03 2.29716623e+00 1.18918698e-01]\n",
      "  [6.21097625e-03 2.63839960e-04 2.30626839e+00 1.21483343e-01]\n",
      "  [6.52895152e-03 1.28120184e-04 2.36425340e+00 1.21108558e-01]\n",
      "  [7.01826315e-03 3.95774841e-04 2.65646088e+00 1.22079937e-01]\n",
      "  [4.41637178e-03 1.88311934e-03 2.20986557e+00 1.18006491e-01]\n",
      "  [5.88593952e-03 7.00950623e-04 2.22925919e+00 1.20547141e-01]\n",
      "  [6.66986086e-03 5.59568405e-04 2.52239716e+00 1.21260529e-01]\n",
      "  [6.01775286e-03 1.18857622e-03 2.40633684e+00 1.20019408e-01]]] [[[ 9.83227439e-01  7.89217293e-01  1.46412182e+00 -7.13628194e-03]\n",
      "  [ 9.82935322e-01  7.88994491e-01  1.46164918e+00 -6.53794143e-03]\n",
      "  [ 9.80429356e-01  7.87249982e-01  1.41155612e+00 -1.41220140e-03]\n",
      "  [ 9.79090752e-01  7.85061002e-01  1.46795523e+00  1.32043794e-03]\n",
      "  [ 9.78163330e-01  7.86626399e-01  1.24945009e+00  3.21143303e-03]\n",
      "  [ 9.82148544e-01  7.88419545e-01  1.45018387e+00 -4.92725819e-03]\n",
      "  [ 9.77348073e-01  7.85147309e-01  1.31556582e+00  4.87244910e-03]\n",
      "  [ 9.78802089e-01  7.86583245e-01  1.32645261e+00  1.90923898e-03]\n",
      "  [ 9.79928373e-01  7.86583006e-01  1.43706667e+00 -3.89056387e-04]]\n",
      "\n",
      " [[ 9.69178536e-01  7.70384371e-01  1.31587589e+00  2.07192184e-02]\n",
      "  [ 9.69399788e-01  7.70154834e-01  1.33399940e+00  2.15284247e-02]\n",
      "  [ 9.69973561e-01  7.73409188e-01  1.31775296e+00  1.55222014e-02]\n",
      "  [ 9.65716843e-01  7.70526826e-01  1.57261252e+00  2.27657215e-02]\n",
      "  [ 9.65690859e-01  7.71233737e-01  1.39719748e+00  2.08625474e-02]\n",
      "  [ 9.70457940e-01  7.70119131e-01  1.33953667e+00  2.17351300e-02]\n",
      "  [ 9.68770396e-01  7.70825028e-01  1.29145777e+00  1.92330254e-02]\n",
      "  [ 9.65171974e-01  7.70455003e-01  1.72593331e+00  2.32909493e-02]\n",
      "  [ 9.65425324e-01  7.68952787e-01  1.74070728e+00  2.53876832e-02]]] [[[0.99717361 0.8055746  2.96319056 0.27872404]\n",
      "  [0.99721365 0.80553806 2.99269819 0.2786662 ]\n",
      "  [0.9959729  0.8046813  3.02469516 0.28046003]\n",
      "  [0.99528486 0.80409718 3.16278172 0.28145388]\n",
      "  [1.00019161 0.80822659 3.11598778 0.2743515 ]\n",
      "  [0.9970102  0.8054052  3.0212431  0.27896043]\n",
      "  [0.99569069 0.80451739 3.0244472  0.28086772]\n",
      "  [0.99941818 0.80787241 3.25411654 0.27547336]\n",
      "  [0.99895611 0.80672401 2.9582839  0.27614313]]\n",
      "\n",
      " [[0.97979911 0.77298111 5.91108513 0.25970396]\n",
      "  [0.97939421 0.77329028 5.92833185 0.25936582]\n",
      "  [0.98239551 0.77393687 5.93028975 0.25848889]\n",
      "  [0.97877475 0.77078307 6.30111933 0.26498284]\n",
      "  [0.97972738 0.77202529 6.71011925 0.26502242]\n",
      "  [0.97929068 0.77388537 5.75926781 0.25774811]\n",
      "  [0.98054228 0.77222693 5.74997616 0.26032731]\n",
      "  [0.9785117  0.77157414 6.77072763 0.26581201]\n",
      "  [0.97746083 0.77132994 6.55338097 0.2654265 ]]]\n",
      "(2, 9, 4) (2, 9, 4) (2, 9, 4) (2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "with np.load(path+'Results/'+setting+'_'+str_model+'_results.npz') as data:\n",
    "    print(data['mean'].shape, data['std'].shape, data['minimum'].shape, data['maximum'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ebd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
